<!DOCTYPE html>
<!-- TypeIt package -->
<script src="https://code.jquery.com/jquery-3.0.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/jquery.typeit/4.4.0/typeit.min.js"></script>
<html>
    <head>
        <meta charset="utf-8">
        <meta name="description" content="Endoscopic-image-based Flexible Robot Pose Estimation">
        <meta name="keywords" content="Flexible Robot, Pose Estimation, Shape-guided Model">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <title>Shape-guided Configuration-aware Learning for Endoscopic-image-based Pose Estimation of Flexible Robotic Instruments</title>
        <!-- Google tag (gtag.js) -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=G-MK2R9XDD88"></script>
        <script>
            window.dataLayer = window.dataLayer || [];
            function gtag() { dataLayer.push(arguments); }
            gtag('js', new Date());

            gtag('config', 'G-MK2R9XDD88');
        </script>
        <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
        <link rel="stylesheet" href="./static/css/bulma.min.css">
        <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
        <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
        <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
        <link rel="stylesheet" href="./static/css/index.css">
        <link rel="icon" href="./static/paper_images/icon.png">
        <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
        <script defer src="./static/js/fontawesome.all.min.js"></script>
        <script src="./static/js/bulma-carousel.min.js"></script>
        <script src="./static/js/bulma-slider.min.js"></script>
        <script src="./static/js/index.js"></script>
        <!-- Typing Effect JS -->
        <script src="https://code.jquery.com/jquery-3.0.0.min.js"></script>
        <script src="https://cdn.jsdelivr.net/jquery.typeit/4.4.0/typeit.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/typeit/5.10.1/typeit.min.js"></script>
        <!-- / Typing Effect JS -->
        <style>
    .bigdiv {
      font-size: large;
      font-family: "Courier New";
      padding: 2rem;
    }
    p {
      padding: 2rem;
    }
        </style>
    </head>
    <body>
        <section class="hero">
            <div class="hero-body">
                <div class="container is-max-desktop">
                    <div class="columns is-centered">
                        <div class="column has-text-centered">
                            <h1 class="title is-2 publication-title">Shape-guided Configuration-aware Learning <br> for Endoscopic-image-based Pose Estimation <br> of Flexible Robotic Instruments</h1>
                            <h3 class="title is-4 conference-authors">
                                <a target="_blank" href="https://eccv.ecva.net/">
                                    ECCV 2024
                                </a>
                            </h3>
                            <div class="is-size-5 publication-authors">
                                <span class="author-block">
                                    <span>Yiyao Ma</span>
                                    <sup>1*</sup>
                                    ,
                                </span>
                                <span class="author-block">
                                    <a href="https://ck-kai.github.io/">Kai Chen</a>
                                    <sup>1*</sup>
                                    ,
                                </span>
                                <span class="author-block">
                                    <span>Hon-Sing Tong</span>
                                    <sup>2</sup>
                                    ,
                                </span>
                                <span class="author-block">
                                    <span>Ruofeng Wei</span>
                                    <sup>1</sup>
                                    ,
                                </span>
                                <br>
                                <span class="author-block">
                                    <span>Yui-Lun Ng</span>
                                    <sup>2</sup>
                                    ,
                                </span>
                                <span class="author-block">
                                    <span>Ka-Wai Kwok</span>
                                    <sup>1,2,3†</sup>
                                    , and
                                </span>
                                <span class="author-block">
                                    <a href="https://www.cse.cuhk.edu.hk/~qdou/index.html">Qi Dou</a>
                                    <sup>1†</sup>
                                </span>
                            </div>
                            <div class="is-size-5 publication-authors">
                                <span class="author-block">
                                    <sup>1</sup>The Chinese University of Hong Kong,
                                </span>
                                <span class="author-block">
                                    <sup>2</sup>Agilis Robotics Limited,
                                </span>
                                <span class="author-block">
                                    <sup>3</sup>The University of Hong Kong
                                </span>
                            </div>
                            <div class="is-size-5 publication-authors">
                                <span class="author-block" style="font-size: 15px;">(
                                    <sup>*</sup>Equal Contribution, <sup>†</sup>Corresponding Authors)
                                </span>
                            </div>
                            <div class="column has-text-centered">
                                <div class="publication-links">
                                    <!-- PDF Link. -->
                                    <span class="link-block">
                                        <a href="https://poseflex.github.io/" class="external-link button is-normal is-rounded is-dark">
                                            <span class="icon">
                                                <i class="fas fa-file-pdf"></i>
                                            </span>
                                            <span>Paper <em>(coming soon)</em></span>
                                        </a>
                                    </span>
                                    <!-- Code Link. -->
                                    <span class="link-block">
                                        <a href="https://poseflex.github.io/" class="external-link button is-normal is-rounded is-dark">
                                            <span class="icon">
                                                <i class="fab fa-github"></i>
                                            </span>
                                            <span>Code <em>(coming soon)</em></span>
                                        </a>
                                    </span>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </section>
            <div class="content has-text-centered">
              <img src="./static/paper_images/figure_teaser.png" style="width:800px;">
              <div class="content is-centered" style="color: gray; font-size: 10pt;">
                <b>Figure 1</b>.
                Illustration of a flexible robot with four pose parameters.
            </div>
            </div>
            <section class="section">
                <div class="container is-max-desktop">
                    <!-- Abstract. -->
                    <div class="columns is-centered has-text-centered">
                        <div class="column is-four-fifths">
                            <h2 class="title is-3">Abstract</h2>
                            <div class="content has-text-justified">
                                <p>
                                    Accurate estimation of both the external orientation and internal bending angle is crucial for understanding a flexible robot state within its environment. 
  However, existing sensor-based methods face limitations in cost, environmental constraints, and integration issues. 
  Conventional image-based methods struggle with the shape complexity of flexible robots.
  In this paper, we propose a novel shape-guided configuration-aware learning framework for image-based flexible robot pose estimation. 
  Inspired by the recent advances in 2D-3D joint representation learning, we leverage the 3D shape prior of the flexible robot to enhance its image-based shape representation.
  Concretely, we first extract the part-level geometry representation of the 3D shape prior, then adapt this representation to the image by querying the image features corresponding to different robot parts.
  Furthermore, we present an effective mechanism to dynamically deform the shape prior.
  It aims to mitigate the shape difference between the adopted shape prior and the flexible robot depicted in the image.
  This more expressive shape guidance further boosts the image-based robot representation and can be effectively used for flexible robot pose refinement.
  Extensive experiments on surgical flexible robots demonstrate the advantages of our method when compared with a series of keypoint-based, skeleton-based and direct regression-based methods.
                                </p>
                            </div>
                        </div>
                    </div>
                    <!--/ Abstract. -->
                </div>
            </section>
            
            <section class="section">
                <div class="container is-max-desktop">
                    <div class="columns is-centered has-text-centered">
                        <div class="column is-four-fifths">
                            <h2 class="title is-3">Flexible Robot Pose Estimation</h2>
                            <h3 class="title is-4">Pose Estimation with Configuration-aware Shape Guidance</h3>
                            <div class="content has-text-justified">
                                <!-- Add image -->
                                <div class="figure" style="align: left; text-align:center;">
                                    <img
                                        src="./static/paper_images/method1.png"
                                        alt="img description"
                                        width="800"
                                        height="600"
                                    >
                                    <div class="content is-centered" style="color: gray; font-size: 10pt;">
                                        <b>Figure 2</b>. Illustration of the shape-guided configuration-aware learning method.
                                    </div>
                                    <br>
                                </div>
                                Based on the flexible robot shape prior and its part labels derived from its configuration information, we extract a part-level flexible robot shape representation.
                                This representation is then utilized to enhance the image-based flexible robot representation for improving the accuracy of image-based flexible robot pose estimation.
                                To parameterize the flexible robot pose, we employ a probabilistic model that simultaneously predicts both the pose value and the pose uncertainty.
                            </div>
                            <br>
                            <h3 class="title is-4">Pose Refinement with Configuration-aware Shape Deformation</h3>
                            <div class="content has-text-justified">
                                <!-- Add image -->
                                <div class="figure" style="align: left; text-align:center;">
                                    <img
                                        src="./static/paper_images/method2.png"
                                        alt="img description"
                                        width="800"
                                        height="600"
                                    >
                                    <div class="content is-centered" style="color: gray; font-size: 10pt;">
                                        <b>Figure 3</b>. Illustration of the pose refinement scheme.
                                    </div>
                                    <br>
                                </div>
                                Based on the initial flexible robot pose, we deform the robot shape prior via <b>skeleton curve modeling</b> and <b>cylinder instantiation</b>.
                                <!-- <ul>
                                    <li>
                                        <b>(a) Skeleton curve modeling.</b>
                                        Under the Piecewise Constant Curvature (PCC) assumption, we can determine the skeleton curve using a set of control points.

                                    </li>
                                    <li>
                                        <b>(b) Cylinder instantiation.</b>
                                        Based on the candidate factors, COAT
                then prompts another LLM to annotate or fetch the structured values of the unstructured data. With the annotated
                structured data.
                                    </li>
                                </ul> -->
                            </div>
                        </div>
                    </div>       
                </div>
            </section>
        <!--new section-->
        <section class="section">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column is-full-width has-text-centered">
                        <h2 class="title is-3">Results</h2>
                        <div class="content has-text-justified is-centered">
                            <b><h5>Does the proposed method surpass existing image-based approaches?</h5></b>
                            It can be found that 
                            conventional keypoint-based (<b>KP</b>) and skeleton-based (<b>SKL</b>) methods yield poor performance 
                            when attempting to localize keypoints and extract complete skeletons from high degrees of freedom (DoF) flexible robots.
                            Although regression-based (<b>DR</b>, <b>SimPS</b>) methods outperform <b>KP</b> and <b>SKL</b>, 
                            they still fall short due to a lack of an effective mechanism to model the variation in flexible robot shapes.

                            In contrast, our method <b>PoseEst.</b> leverages the informative shape guidance to enhance the flexible robot shape representation, 
                            and <b>PoseRefine.</b> further improves the representation by deforming the flexible robot shape with the initial pose parameters. 
                            These strategies significantly improve the accuracy of pose estimation.
                        </div>
                        <div class="content has-text-centered">
                            <img
                                src="./static/paper_images/table1.png"
                                style="width:600px;"
                                class="result-image"
                                alt="Interpolation end reference image."
                            >
                            <div class="content has-text-justified is-centered" style="color: gray; font-size: 10pt;">
                                <b>Table 1</b>.
                                Quantitative comparison between our methods with the state-of-the-art methods.
                                We reported both average (Mean) and median (Med.) angular errors for each of the predicted pose parameters,
                                and the ratio of predictions whose prediction error is smaller than 5° (Acc5°) or 10°(Acc10°).
                                The initial pose of PoseRefine. comes from the results of PoseEst.
                            </div>
                            
                        </div>
                        <div class="content has-text-centered">
                            <img
                                src="./static/paper_images/figure_qualitative.png"
                                style="width:600px;"
                                class="result-image"
                                alt="Interpolation end reference image."
                            >
                            <div class="content is-centered" style="color: gray; font-size: 10pt;">
                                <b>Figure 4</b>.
                                Qualitative comparison between our methods with the state-of-the-art methods.
                            </div>
                        </div>
                        <br>
                        <div class="content has-text-justified is-centered">
                            <b><h5>How effective is the shape guidance in enhancing the accuracy of pose estimation?</h5></b>
                            The leverage of the shape guidance can reduce the prediction error for most pose parameters.
                            In addition, the model with shape guidance consistently achieves higher accuracy with respect to different error thresholds.
                            Removing the robot configuration information from the shape guidance would consistently degrade the pose accuracy.
                        </div>
                        <div class="content has-text-centered">
                            <img
                                src="./static/paper_images/figure_shape_guidance.png"
                                style="width:500px;"
                                class="result-image"
                                alt="Interpolation end reference image."
                            >
                            <div class="content is-centered" style="color: gray; font-size: 10pt;">
                                <b>Figure 5</b>. Ablation on shape guidance.
                            </div>
                        </div>
                        <br>
                        <div class="content has-text-justified is-centered">
                            <b><h5>Does the shape prior guidance outperforms depth-based counterparts?</h5></b>
                            We recover the depth map from the image with a pre-trained depth prediction Transformer, lift the flexible robot to 3D, 
                            and extract the geometry feature from the robot point cloud for pose estimation.
                            In both estimation and refinement stages, the depth-based counterpart is inferior to ours, 
                            primarily attributed to the severe shape distortions caused by depth noise.
                        </div>
                        <div class="content has-text-centered">
                            <img
                                src="./static/paper_images/figure_depth_compare.png"
                                style="width:500px;"
                                class="result-image"
                                alt="Interpolation end reference image."
                            >
                            <div class="content is-centered" style="color: gray; font-size: 10pt;">
                                <b>Figure 5</b>. Comparison with depth-based counterparts.
                            </div>
                        </div>
                        <div class="content has-text-centered">
                            <img
                                src="./static/paper_images/figure_depth_dpt.png"
                                style="width:600px;"
                                class="result-image"
                                alt="Interpolation end reference image."
                            >
                            <div class="content is-centered" style="color: gray; font-size: 10pt;">
                                <b>Figure 6</b>. 
                                Illustration of the depth map and reconstructed point cloud.
                            </div>
                            <br>
                        </div>
                        <div class="content has-text-justified is-centered">
                            <b><h5>Is the proposed methods can be applied to flexible robots with diverse configurations?</h5></b>
                            We made modifications on the robot arm by varying the arm thickness (Thick.), arm length (Len.), and the number of segments (Num.). 
                            It demonstrates that our methods can smoothly adapt to robots with diverse configurations and surpass the most competitive baseline in the main result.
                        </div>
                        <div class="content has-text-centered">
                            <img
                                src="./static/paper_images/table2.png"
                                style="width:600px;"
                                class="result-image"
                                alt="Interpolation end reference image."
                            >
                            <div class="content has-text-justified is-centered" style="color: gray; font-size: 10pt;">
                                <b>Table 2</b>.
                                Quantitative evaluation on flexible robots with diverse configurations and results under different environmental changes.
                            </div>
                            <br>
                        </div>
                        <div class="content has-text-justified is-centered">
                            <b><h5>Is the proposed method robust under various challenging surgical environments?</h5></b>
                            We conducted experiments under challenging visual conditions that typically present in surgery,
                            including too bright or dark lighting conditions (Lighting), visual occlusions caused by flushing water and bubbles (Occlusion), 
                            and image blur caused by robot motion (Scope Rot.).
                            With the help of 3D shape guidance, our method keeps commendable performance in these challenging scenarios.
                        </div>
                        <div class="content has-text-centered">
                            <img
                                src="./static/paper_images/figure_robustness.png"
                                style="width:700px;"
                                class="result-image"
                                alt="Interpolation end reference image."
                            >
                            <div class="content is-centered" style="color: gray; font-size: 10pt;">
                                <b>Figure 7</b>.
                                Qualitative results under environmental changes.
                            </div>
                            <br>
                        </div>
                        <div class="content has-text-justified is-centered">
                            <b><h5>Could the pose refinement method be generally effective?</h5></b>
                            We evaluated the model in two different scenarios.
                            First, we used it to refine the pose prediction from other baseline methods (Figure 8).
                            Second, we take the pose prediction from the previous frame as the initial robot pose for the current frame,
                            which is similar to robot pose tracking (Figure 9).
                            The results indicate that the pose refinement model can significantly improve the average accuracy 
                            as well as the prediction robustness within the whole sequence.
                        </div>
                        <div class="content has-text-centered">
                            <img
                                src="./static/paper_images/figure_pose_refine1.png"
                                style="width:600px;"
                                class="result-image"
                                alt="Interpolation end reference image."
                            >
                            <div class="content is-centered" style="color: gray; font-size: 10pt;">
                                <b>Figure 8</b>.
                                Effectiveness of pose refinement.
                            </div>
                        </div>
                        <div class="content has-text-centered">
                            <img
                                src="./static/paper_images/figure_pose_refine2.png"
                                style="width:600px;"
                                class="result-image"
                                alt="Interpolation end reference image."
                            >
                            <div class="content has-text-justified is-centered" style="color: gray; font-size: 10pt;">
                                <b>Figure 9</b>.
                                Comparative results of flexible robot pose tracking.
                                Blue and red points represent ground truth and model predictions, respectively.
                            </div>
                            <br>
                        </div>
                        <div class="content has-text-justified is-centered">
                            <b><h5>Could the uncertainty value reflect the pose estimation quality?</h5></b>
                            We adopt matrix Fisher distribution to construct a probabilistic model for representing 
                            rotation matrices and improving pose estimation.
                            It can provide both the pose estimation and the reliance of the prediction.
                            The results suggest that data with greater uncertainty are more likely to have larger errors, 
                            verifying that the uncertainty can be an effective indicator to reflect the pose quality.
                        </div>
                        <div class="content has-text-centered">
                            <img
                                src="./static/paper_images/figure_uncertainty.png"
                                style="width:600px;"
                                class="result-image"
                                alt="Interpolation end reference image."
                            >
                            <div class="content has-text-justified is-centered" style="color: gray; font-size: 10pt;">
                                <b>Figure 10</b>.
                                Indication ability of uncertainty with model performance.
                                (a) The x axis is the uncertainty value, and the y axis is the number of data points. 
                                Different colors represent different error ranges, in degree.
                                (b) Qualitative results of the data with different error ranges and uncertainty values. 
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </section>
        <section class="section" id="BibTeX">
            <div class="container is-max-desktop content">
                <h2 class="title">Contact</h2>
                Welcome to check our paper for more details of the research work. 
                If there is any question, please feel free to contact us. </br>
                If you find our paper and repo useful, please consider to cite:
                <pre>
                    <code>
                    TODO: Add citation here.
                    </code>
                </pre>
                <br>
            </div>
        </section>
        <footer class="footer">
            <div class="container">
                <!-- <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div> -->
                <div class="columns is-centered">
                    <div class="column is-8">
                        <div class="content">
                            <p>
                                Thanks for the source template from
                                <a href="https://github.com/nerfies/nerfies.github.io">here</a>
                                .
                            </p>
                        </div>
                    </div>
                </div>
            </div>
        </footer>
